---
title: "Math 437 - Midterm Exam 1"
author: "Vivi B. Ngo"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries and data, message = FALSE, warning = FALSE}
# Packages you definitely need
library(ggplot2)
library(dplyr)

library(tidyverse)
library(rstatix)
library(ggpubr)
# Please load any other packages you need either all in this chunk
# or in the chunk you need it for

# Data you need for applied problems
toad_testes <- readr::read_csv("toad_testes.csv")
cats <- readr::read_csv("cats.csv")
```



# Conceptual Problem

## Part a 

In hypothesis testing (significance testing) using Fisher's framework, the p-value represents the posterior probability that the null hypothesis is true, conditional on the data that was observed.

Always false, the definition of the Bayesian approach has a posterior alpha with likelihood of x happening prior . 

## Part b 

We can test $H_0: \mu_1 - \mu_2 = 3$ using a permutation-based two-sample t-test.

Always false, a permutation-based two-sample t-test compares two groups to see if the treatment has any effect.  

## Part c 

If we control the family-wise error rate at $\alpha = 0.05$, and all of our null hypotheses are true, then there is at least a 95% chance that we make no Type I Errors.

Always true, if we set our significant level $\alpha \approx 0 $, we will never make a Type 1 Error

## Part d

Since Tukey's method is more powerful than Bonferroni's method, we should use Tukey's method to control the family-wise error rate instead of Bonferroni's method.

Sometimes true, although Tukey's method is more powerful, it only works when group have equal sample sizes. 

## Part e 

In the long run, 95% of all 95% bootstrap percentile confidence intervals for a population mean will contain the true value of the population mean.

Sometimes true, 95% confidence interval is a range of values that you can be 95% certain contains the true mean of the population, this is not the same as a range that contains 95% of the values.

## Part f 

In a multiple linear regression model, if a predictor has a variance inflation factor (VIF) near 1, then the predictor is not significantly related to the response variable.

Always true, VIF ranges from 1 upwards, the numerical value for VIF tells you the variance for each coefficient, if a predictor has a VIF near 1 then the predictor is not correlated. 

# Simulation Problem 

Consider the following simulation.

```{r simulation 1}
set.seed(1022)
x <- matrix(rnorm(25000, mean = 1, sd = 0.5), nrow = 1000, ncol = 25)
epsilon <- rnorm(1000)
Y <- 10 + 2*x[,1] - 0.5*x[,2] + 0.25*x[,3] - 0.2*x[,4] + 0.3*x[,5] + epsilon

lr.df <- data.frame(Y, x) # create a data frame with response Y and predictors x, 

lm1 <- lm(Y ~ ., data = lr.df)

```

## Part a 


Line 72: we set.seed to randomly generate and store our results to reproduce it. 
Line 73: We created a 1000 by 25 matrix with 25,000 random normal data with mean = 1 and sd = 0.5.
Line 74: We declared a variable epsilon with 1000 random normal data
Line 75: We declared a variable Y to make a response Y with given formula
Line 77: We declared a variable lr.df and created a data frame with response Y and predictors x
Line 79: We declared a variable lm1 to and assigned a linear model that uses the response Y, predictors, and lr.df data 

## Part b 

Store the p-values for the t-tests for the coefficients in the `lm1` model, rounded to 3 decimal places (the function `round` looks appropriate here), in a variable `lm1_pvalues`. Recall that the p-values for these t-tests are equivalent to the p-values for the partial ANOVA tests for testing the `lm1` model against the `lm1` model with that variable removed, but are much more easily obtained in R (you can use `summary(lm1)$coefficients` to output the entire coefficients estimate matrix).

```{r}
lm1_pvalues <-round(summary(lm1)$coefficients[2:26,4], digits = 3)

lm1_pvalues

```


Print your `lm1_pvalues` vector. At the 5% significance level, which variables are significant predictors of the response `Y` (i.e., which predictor variables have slopes significantly different from 0 in the model)?

$x_1,x_2,x_3,x_4,x_5,x_{16},x_{22}$ are significant predictors of the response $Y$.

```{r}
lm1_pvalues <-round(summary(lm1)$coefficients[2:26,4], digits = 3) <= 0.05

lm1_pvalues

```

## Part c 



```{r}

stored_pvalues <- numeric(25)

for (i in 1:25) {
  
  lm_chunk <- lm(Y~lr.df[,i+1])
  stored_pvalues[i] = summary(lm_chunk)$coefficient[2,4]
  
}


```


Adjust the p-values using Holm's step-down method. Controlling the family-wise error rate at $\alpha = 0.10$

Type 1 errors: 2
Type 2 error: 0

```{r}
p.adjusted = p.adjust(stored_pvalues, method = "holm") 
alpha = 0.10
m=26
sig.lvl = (alpha)/(m-(1:25))
rejects = p.adjusted <= sig.lvl

total <- data.frame(p.adjusted, rejects)
```


## Part d 



Type I Error (False when suppose to be True) : 2
Type II Error (False and Actually False): 0 

```{r}

bh.adjusted = p.adjust(stored_pvalues, method = "BH") 
q = 0.20
m=25
sig.lvl = q*(1:25)/m
rejects = bh.adjusted <= sig.lvl

total <- data.frame(bh.adjusted, rejects)


```


# Applied Problem 1 

[Friesen and Shine (2019)](https://royalsocietypublishing.org/doi/pdf/10.1098/rsbl.2019.0339) hypothesized that within a species, individuals at the boundary of its range experience different ecological pressures than individuals at the center of its range. One specific hypothesis they had was that males at the boundary should have smaller testes.

The *toad_testes* file contains the following variables they recorded on 214 Australian cane toads:

* region: the region in which the toad was collected, either **CORE** (center of the range), **W-FRONT** (western boundary of the range), or **S-FRONT** (southern boundary of the range)
* total_mass: the total mass of the left and right testis, in mg

## Part a

Produce an appropriate graph and an appropriate grouped numerical summary for this data. Using this graph and summary, explain why a one-way ANOVA would *not* be appropriate for analyzing this data. Would a permutation-based F test be appropriate to analyze this data?


Looking at the plots generated, there are high outliers, permutation tests are not as susceptible to outliers, which is a more preforable test than the ANOVA test.The one-way ANOVA is less powerful when there are outliers. 
```{r}
mutated_testes <- toad_testes %>% mutate(
  region2 = case_when(
    region == "CORE"~"CORE",
    TRUE ~ "Boundaries"
  )
)

mutated_mass = mutated_testes$total_mass
mutated_region2 = mutated_testes$region2

plot(total_mass~as.factor(region2), data = mutated_testes, xlab = "Region", ylab = "Total Mass")

summary(mutated_mass,mutated_region2 )

```


## Part b
It turns out that a log-transformation will at least "sort of" fix the problems you identified in part (a). Using the `log10` function, create a new variable, `log_mass`, containing the base 10 log of the testes mass.

Using the `log_mass` variable, perform a one-way ANOVA to test $H_0:$ the population mean testes mass is the same in all three regions. Use the output to write a conclusion.

Based on the summary, the log-transformation "sort of" fixes the outlier problem. 

```{r}
log_mass <- log10(toad_testes$total_mass)

mass_aov <- summary(aov(log_mass~region, data = toad_testes))

mass_aov2 <- summary(aov(total_mass~region, data = toad_testes))


```



## Part c 

Recall that the original hypothesis was that toads at the boundary of the range should have smaller testes mass than toads at the center of the range. Use an appropriate test to determine whether they have sufficient evidence, at the $\alpha = 0.05$ significance level, to make this claim.

We obtained a small p-value based on our test we can conclude that the results of this experiment is statistically significant at an $\alpha = 0.05$.
There is significant evidence  to suggest that boundary toads has smaller testes than toads at the center of the range. 

```{r}
t.output <- t.test(total_mass~as.factor(region2), data = mutated_testes)
t.output
```


## Part d 

Obtain a 99% confidence interval for the population mean of `testes_mass` for toads in the **CORE** region. Use any bootstrap method of your choice with 10,000 bootstrap replicates.

```{r}

set.seed(1337)
n.data <- nrow(toad_testes)
mass.mean <- mean(toad_testes$total_mass%>%filter(toad_testes$region==CORE))
#mass.mean <- mean(toad_testes$total_mass)
#mass.mean%>%filter(toad_testes$region==CORE)
#[toad_testes$region==CORE]
B <- 10000
mass.boot <-numeric(B)

for (i in 1:B) {
  
  resample = sample(toad_testes$total_mass, size = n.data, replace = TRUE)
  
  mass.boot[i]=mean(resample)
  
}

C<-.99
alpha = 1 - C
mass.boot.pctile <-quantile(mass.boot, probs = c(alpha/2, 1 - alpha/2))
mass.boot.pctile

```

# Applied Problem 

The *cats* dataset on Canvas contains measurements on 101 cats performed by [Lesch and colleagues (2022)](https://royalsocietypublishing.org/doi/10.1098/rsos.210477). The variables in this dataset are:

* ID: the National Museum of Scotland ID for the cat
* Species: the species to which the cat belonged: *catus* (domestic cat), *silvestris* (European wildcat), *lybica* (African wildcat), or a *hybrid* of domestic and wild cats
* BrainVolume: the volume of the cat's brain, in grams
* PalateLength: the length of the palate (mouth/snout), in mm
* BasalSkullLength: the length of the skull, in mm

## Part a 
* How much data is missing?
We have 5 N/A data points.

* What is the distribution of each (non-ID) variable in the dataset? (For numerical variables: consider describing the distribution in terms of center, variability, shape, and outliers)

BV seems linear and normal, 

* Do any of the variables appear to be related?

Yes the variables do appear to be related, the variables have a linear relationship with each other

* Are there "hidden" relationships between the numerical variables that only show up when you look at each species separately (i.e., using different colors/shapes to differentiate the species)? If so, what do you see?

```{r}


BV = cats$BrainVolume
PL = cats$PalateLength
BSL = cats$BasalSkullLength
Specs = cats$Species

pairs(~BV+PL+BSL)

summary(cats)


boxplot(BV,PL)
boxplot(BSL,PL)
boxplot(BV,BSL)

catdata <- data.frame(BV,PL,BSL,Specs)
sum(is.na(catadata))



```


## Part b 

We can see as BasallSkullLength increases, the BrainVolume increases, therefore cats that are `Speciessilvestris` have larger BrainVolume compared to other species. 

```{r}
model1 <- lm(BV~BSL+Specs, data=cats)
summary(model1)

```


## Part c (Code: 2 pts; Explanation: 2 pts)

Fit a model predicting brain volume from basal skull length, species, and the interaction between the two variables. Use the coefficient estimates to write out four simple linear regression equations predicting brain volume from basal skull length (one for each species). Round the coefficients in each equation to a reasonable number of decimal places.

```{r}
model2 <- lm(BV~BSL+Specs+BSL*Specs, data=cats)
summary(model2)
```


hybrid: y = -1.738 + 0.667*BSL
lybica: y = 3.221 + 0.546*BSL
silvestris: y = 231.635+.5*BSL
catus: y = 8.613 + 0.420*BSL


I would prefer to use the linear model with the interaction term, performing an AOV test with $H_0 = \mu_1=\mu_2=\mu_3$, the summary shows there a very strong correlation between the three variables. 


```{r}
summary(aov(BSL~as.factor(Specs)))
summary(aov(BV~BSL))
summary(aov(BV~as.factor(Specs)))
```


## Part e 
model 1 $R^2$ is 0.796, model 3 $R^2$ 0.5275, model 4 is  0.796. Our hypothesis test above showed us there is a correlation between the three variables, the paradox could be that PalateLength does not add any emphasis on our model, yet at the same time it does. model 1 has the same $R^2$ as model 4. 
```{r}
model1
summary(model1)

model3 <- lm(BV~PL)
summary(model3)

model4 <- lm(BV~BSL+Specs+PL)
summary(model4)
```

